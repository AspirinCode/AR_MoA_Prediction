{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_all before removing low variance features: (7665, 17967)\n",
      "Shape of X_all after removing low variance features: (7665, 2544)\n",
      "Testing on fold 1 of 5\n",
      "fmeasure: 0.91%\n",
      "[[ 27   0   1   0]\n",
      " [  1  11   0   5]\n",
      " [  0   1 954  39]\n",
      " [  5   2  86 403]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.96      0.89        28\n",
      "          1       0.79      0.65      0.71        17\n",
      "          2       0.92      0.96      0.94       994\n",
      "          3       0.90      0.81      0.85       496\n",
      "\n",
      "avg / total       0.91      0.91      0.91      1535\n",
      "\n",
      "Precision: 0.855472153098\n",
      "Recall: 0.845900772281\n",
      "f1_score: 0.84680861014\n",
      "AUROC: 0.968570135553\n",
      "AUPRC: 0.889869280581\n",
      "Testing on fold 2 of 5\n",
      "fmeasure: 0.91%\n",
      "[[ 25   0   2   1]\n",
      " [  1  10   0   6]\n",
      " [  1   1 953  39]\n",
      " [  6   4  73 412]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.89      0.82        28\n",
      "          1       0.67      0.59      0.62        17\n",
      "          2       0.93      0.96      0.94       994\n",
      "          3       0.90      0.83      0.86       495\n",
      "\n",
      "avg / total       0.91      0.91      0.91      1534\n",
      "\n",
      "Precision: 0.812712136144\n",
      "Recall: 0.818042046097\n",
      "f1_score: 0.812985293704\n",
      "AUROC: 0.962733088531\n",
      "AUPRC: 0.879144316523\n",
      "Testing on fold 3 of 5\n",
      "fmeasure: 0.91%\n",
      "[[ 24   0   1   3]\n",
      " [  2   7   0   7]\n",
      " [  0   0 955  38]\n",
      " [  0   3  79 413]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.86      0.89        28\n",
      "          1       0.70      0.44      0.54        16\n",
      "          2       0.92      0.96      0.94       993\n",
      "          3       0.90      0.83      0.86       495\n",
      "\n",
      "avg / total       0.91      0.91      0.91      1532\n",
      "\n",
      "Precision: 0.860415190508\n",
      "Recall: 0.77267960409\n",
      "f1_score: 0.808295439853\n",
      "AUROC: 0.956536854732\n",
      "AUPRC: 0.843322717756\n",
      "Testing on fold 4 of 5\n",
      "fmeasure: 0.91%\n",
      "[[ 24   0   3   1]\n",
      " [  1   9   0   6]\n",
      " [  0   1 955  37]\n",
      " [  1   0  84 410]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.86      0.89        28\n",
      "          1       0.90      0.56      0.69        16\n",
      "          2       0.92      0.96      0.94       993\n",
      "          3       0.90      0.83      0.86       495\n",
      "\n",
      "avg / total       0.91      0.91      0.91      1532\n",
      "\n",
      "Precision: 0.910666835342\n",
      "Recall: 0.802414452575\n",
      "f1_score: 0.845959739795\n",
      "AUROC: 0.954282683111\n",
      "AUPRC: 0.88222553737\n",
      "Testing on fold 5 of 5\n",
      "fmeasure: 0.90%\n",
      "[[ 27   0   1   0]\n",
      " [  0   9   2   5]\n",
      " [  0   0 933  60]\n",
      " [  3   1  84 407]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.96      0.93        28\n",
      "          1       0.90      0.56      0.69        16\n",
      "          2       0.91      0.94      0.93       993\n",
      "          3       0.86      0.82      0.84       495\n",
      "\n",
      "avg / total       0.90      0.90      0.90      1532\n",
      "\n",
      "Precision: 0.894248504487\n",
      "Recall: 0.822146243946\n",
      "f1_score: 0.848023884187\n",
      "AUROC: 0.955536658822\n",
      "AUPRC: 0.883158378352\n",
      "0.91% (+/- 0.01%)\n",
      "[0.90659215360976975, 0.91365032393431878, 0.91295737790687903, 0.91263843314143445, 0.8977474634080892]\n"
     ]
    }
   ],
   "source": [
    "from talos.metrics.keras_metrics import fmeasure\n",
    "from keras.models import load_model\n",
    "\n",
    "#----------------------------------------------------------\n",
    "data = pd.read_csv('conventional_data_unique.csv', index_col='CID')\n",
    "y_ = data['consensus_act']\n",
    "\n",
    "X_all = data.drop(['consensus_act', 'gmin'], axis = 1)\n",
    "X_all.replace([np.inf, -np.inf], np.nan, inplace=True) #replace all infinite values with NaN\n",
    "\n",
    "X_all = X_all.dropna(axis=1, how='all') #Drop any column that has all NaN\n",
    "X_all.fillna(value = X_all.mean(axis=0), inplace=True) #Impute mean in place on NaN\n",
    "\n",
    "#X_all.loc[:, X_all.isnull().any()] See which column still had NaN after prepocessing. gmin was removed\n",
    "print('Shape of X_all before removing low variance features:', X_all.shape)\n",
    "\n",
    "#0.16 =(.8 * (1 - .8)) that is the threshold for features that are constant in 80% of the instances\n",
    "feat = VarianceThreshold(threshold=0.16)\n",
    "feat.fit_transform(X_all)\n",
    "X_all = X_all[X_all.columns[feat.get_support(indices=True)]] #retain column names\n",
    "print('Shape of X_all after removing low variance features:', X_all.shape)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "n_splits=5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=23)\n",
    "cvscores=[]\n",
    "_model = load_model('conv_feat_unique_model_val.h5', custom_objects={'fmeasure': fmeasure})\n",
    "\n",
    "Predictions_DF = pd.DataFrame()\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all, y_)):\n",
    "    print(\"Testing on fold \" + str(fold+1) + \" of \" + str(n_splits))\n",
    "\n",
    "    xtrain, xval = X_all.iloc[train_idx], X_all.iloc[val_idx]\n",
    "    ytrain, yval = y_.values[train_idx], y_.values[val_idx] #Dropped indexes \n",
    "    \n",
    "    ytrain = pd.get_dummies(ytrain)\n",
    "    yval = pd.get_dummies(yval)\n",
    "    \n",
    "    sc = StandardScaler().fit(xtrain)\n",
    "    xtrain = pd.DataFrame(sc.transform(xtrain), columns = X_all.columns, index=xtrain.index)\n",
    "    xval = pd.DataFrame(sc.transform(xval), columns = X_all.columns, index=xval.index)\n",
    "    \n",
    "    #_model = None\n",
    "    #_model = load_model('docking_feat_full_model_v2.h5', custom_objects={'fmeasure': fmeasure})\n",
    "\n",
    "    scores = _model.evaluate(xval, yval, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (_model.metrics_names[1], scores[1]))\n",
    "    cvscores.append(scores[1])\n",
    "    \n",
    "    val_class = np.argmax(yval.values, axis=1)\n",
    "    val_pred = _model.predict_classes(xval)\n",
    "    #print(confusion_matrix(val_class, val_pred))\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    #labels = ['active_agonist', 'active_antagonist', 'inactive', 'inconclusive']\n",
    "    print(confusion_matrix(val_class, val_pred))\n",
    "    print(classification_report(val_class, val_pred))\n",
    "    print (\"Precision:\", metrics.precision_score(val_class, val_pred, average=\"macro\"))\n",
    "    print (\"Recall:\", metrics.recall_score(val_class, val_pred, average=\"macro\"))\n",
    "    print (\"f1_score:\", metrics.f1_score(val_class, val_pred, average=\"macro\"))\n",
    "    f1_macro_all_features = metrics.f1_score(val_class, val_pred, average=\"macro\")\n",
    "    #------------------------------------------------\n",
    "    y_proba = _model.predict_proba(xval)\n",
    "    print('AUROC:', metrics.roc_auc_score(yval, y_proba, average=\"macro\"))\n",
    "    print('AUPRC:', metrics.average_precision_score(yval, y_proba, average=\"macro\"))\n",
    "    #-----------------------------------------------------------------------------\n",
    "    \n",
    "    #-------------------------------------------------------------------\n",
    "    #Feature Relevance\n",
    "    #How is the metric affected by eliminating a feature (LOO)\n",
    "    features_table = []\n",
    "    all_features_f1 = f1_macro_all_features\n",
    "\n",
    "    for col in xval.columns:\n",
    "        X_modify_test = xval.copy()\n",
    "        X_modify_test[col] = 0\n",
    "        y_predi = _model.predict_classes(X_modify_test)\n",
    "        y_test_ = np.argmax(yval.values, axis=1)\n",
    "        f1 = metrics.f1_score(y_test_, y_predi, average=\"macro\")\n",
    "\n",
    "        aa = []\n",
    "        aa.append(col)\n",
    "        bb = all_features_f1 - f1\n",
    "        aa.append(bb)\n",
    "        aa.append(f1) \n",
    "        features_table.append(aa)\n",
    "\n",
    "    feat_DF2 = pd.DataFrame(features_table, columns = ['Features', 'f1_delta', 'f1'])\n",
    "    feat_DF_ = feat_DF2.sort_values('f1_delta', ascending = False)\n",
    "    feat_DF_.head()\n",
    "    feat_DF_.to_csv('Unique_feat_DL_Run_' + str(fold+1) + '.csv')\n",
    "    #----------------------------------------------------------------------------\n",
    "    #Get Predictions\n",
    "    compare = pd.DataFrame(np.hstack((val_class.reshape(val_class.shape[0],1),\n",
    "                              val_pred.reshape(val_pred.shape[0],1))),\n",
    "                              columns = ['consensus_act', 'predicted_act'], index=xval.index)\n",
    "\n",
    "    compare['score'] = np.where(compare['consensus_act'] == compare['predicted_act'], 'correct', 'incorrect')\n",
    "    #compare['score'] = compare.apply(lambda x: 'correct' if x['consensus_act'] == x['predicted_act'] else 'incorrect', axis=1)\n",
    "    compare.to_csv('Unique_DL_Predictions_Run'+str(fold+1) + '.csv')\n",
    "    \n",
    "    Predictions_DF = Predictions_DF.append(compare)\n",
    "\n",
    "Predictions_DF.to_csv('Unique_DL_Predictions.csv')\n",
    "#---------------------------------------------------------------------------\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))  \n",
    "print(cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7665, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consensus_act</th>\n",
       "      <th>predicted_act</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     consensus_act  predicted_act      score\n",
       "CID                                         \n",
       "177              3              2  incorrect\n",
       "178              2              2    correct\n",
       "227              2              2    correct\n",
       "240              2              2    correct\n",
       "244              2              2    correct"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6133, 2544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 127,    0,    8,    5],\n",
       "       [   5,   46,    2,   29],\n",
       "       [   1,    3, 4750,  213],\n",
       "       [  15,   10,  406, 2045]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Predictions_DF['consensus_act'], Predictions_DF['predicted_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
